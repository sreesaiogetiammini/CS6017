{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "Adapted from *COMP 5360 / MATH 4100, University of Utah, http://datasciencecourse.net/*\n",
    "\n",
    "We continue lecture 13 but switch from using the [statsmodels](http://statsmodels.sourceforge.net/) library to the [scikit-learn](http://scikit-learn.org/) library. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#%matplotlib notebook \n",
    "# uncomment this to get rotatable 3D figures\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.datasets import make_moons, load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "# At least on PC, figure() works to set size, and params does not...\n",
    "plt.figure( figsize=(14,6) )\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example: Classifying Two Moons\n",
    "\n",
    "Let's consider a synthetic dataset in the shape of \"two moons\". Here, each sample has two pieces of information: \n",
    "* the *features*, denoted by $x_i$, which are just a two-dimensional coordinate and \n",
    "* a *class*, denoted by $y_i$, which is either 0 and 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# there are two features contained in X and the labels are contained in y\n",
    "X,y = make_moons(n_samples=500,random_state=1,noise=0.2)\n",
    "\n",
    "# X is a 500x2 numpy.ndarray containing the coordinates for each sample\n",
    "# y is a 500x1 numpy.ndarray containing the class for each sample\n",
    "\n",
    "# print(np.concatenate((X,y[:, np.newaxis]),axis=1)[:10,:])\n",
    "\n",
    "# Plot the data, color by class\n",
    "plt.scatter( X[y == 1, 0], X[y == 1, 1], color=\"DarkBlue\", marker=\"s\",label=\"class 1\" )\n",
    "plt.scatter( X[y == 0, 0], X[y == 0, 1], color=\"DarkRed\", marker=\"o\",label=\"class 2\" )\n",
    "plt.legend( scatterpoints=1 )\n",
    "\n",
    "x_min, x_max = X[:,0].min()  - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "\n",
    "plt.xlim(x_min, x_max)\n",
    "plt.ylim(y_min, y_max)\n",
    "plt.title('Two Moons Test Dataset')\n",
    "plt.xlabel('feature 1')\n",
    "plt.ylabel('feature 2')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Recall that the goal in classification is to develop a rule for classifying the points.\n",
    "\n",
    "Let's see how to use  [scikit-learn](http://scikit-learn.org/) for logistic regression. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# set up the model \n",
    "# we could specify additional parameters here, but we'll just use the default ones\n",
    "model = LogisticRegression() \n",
    "\n",
    "# use the model to fit the data\n",
    "r = model.fit( X, y )\n",
    "\n",
    "print( \"r^2:\", r.score( X, y ) )\n",
    "\n",
    "# Plot the data, color by class\n",
    "plt.scatter(X[y == 1, 0], X[y == 1, 1], color=\"DarkBlue\", marker=\"s\",label=\"class 1\")\n",
    "plt.scatter(X[y == 0, 0], X[y == 0, 1], color=\"DarkRed\", marker=\"o\",label=\"class 2\")\n",
    "plt.legend(scatterpoints=1)\n",
    "\n",
    "# Plot the predictions made by Logistic Regression\n",
    "xx, yy = np.meshgrid(np.linspace(x_min, x_max, 200),np.linspace(y_min, y_max, 200))\n",
    "zz = model.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
    "plt.contourf(xx, yy, zz, cmap=ListedColormap(['DarkRed', 'DarkBlue']), alpha=.2)\n",
    "plt.contour(xx, yy, zz, colors=\"black\", alpha=1, linewidths=0.2) \n",
    "\n",
    "plt.xlim(x_min, x_max)\n",
    "plt.ylim(y_min, y_max)\n",
    "plt.title('Classification of Two Moons using Logistic Regression')\n",
    "plt.xlabel('feature 1')\n",
    "plt.ylabel('feature 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# set up the model \n",
    "# we could specify adittinoal parameters here, but we'll just use the default ones\n",
    "model = LogisticRegression() \n",
    "\n",
    "# use the model to fit the data\n",
    "model.fit(X, y)\n",
    "\n",
    "# Plot the data, color by class\n",
    "#plt.scatter(X[y == 1, 0], X[y == 1, 1], color=\"DarkBlue\", marker=\"s\",label=\"class 1\")\n",
    "#plt.scatter(X[y == 0, 0], X[y == 0, 1], color=\"DarkRed\", marker=\"o\",label=\"class 2\")\n",
    "#plt.legend(scatterpoints=1)\n",
    "\n",
    "# Plot the predictions made by Logistic Regression\n",
    "xx, yy = np.meshgrid(np.linspace(x_min, x_max, 200),np.linspace(y_min, y_max, 200))\n",
    "zz = model.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
    "from mpl_toolkits.mplot3d.axes3d import Axes3D\n",
    "fig = plt.figure(figsize=(14,6))\n",
    "\n",
    "# `ax` is a 3D-aware axis instance, because of the projection='3d' keyword argument to add_subplot\n",
    "ax = fig.add_subplot(1, 2, 1, projection='3d')\n",
    "\n",
    "p = ax.plot_surface(xx, yy, zz)\n",
    "\n",
    "\n",
    "plt.xlim(x_min, x_max)\n",
    "plt.ylim(y_min, y_max)\n",
    "plt.title('Classification of Two Moons using Logistic Regression')\n",
    "plt.xlabel('feature 1')\n",
    "plt.ylabel('feature 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "print('Confusion Matrix:')\n",
    "y_pred = model.predict( X )\n",
    "display( metrics.confusion_matrix(y_true = y, y_pred = y_pred) )\n",
    "\n",
    "print('Accuracy = ', metrics.accuracy_score(y_true = y, y_pred = y_pred) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Another method for classification:  k-Nearest Neighbors (k-NN) \n",
    "\n",
    "**Idea:** To decide the class of a given point, find the k nearest neighbors of that point, and let them \"vote\" on the class. That is, we assign the sample to the class most common among its k nearest neighbors. \n",
    "\n",
    "**Considerations:**\n",
    "1. We must pick k, the number of voting neighbors (typically a small number, say k=10)\n",
    "+ 'Nearest' means closest in distance, so there is some flexibility in defining the distance\n",
    "+ There are different ways to vote. For example, of the k nearest neighbors, I might give the closest ones more weight than farther ones. \n",
    "+ We have to decide how to break ties in the vote. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Dataset: The Iris dataset\n",
    "\n",
    "This dataset was introduced in 1936 by the statistician [Sir Ronald A. Fisher](https://en.wikipedia.org/wiki/Ronald_Fisher). \n",
    "\n",
    "The dataset contains 4 features (attributes) of 50 samples containing 3 different types of iris plants. The goal is to classify the type of iris plant given the attributes. \n",
    "\n",
    "**Features (attributes):**\n",
    "1. sepal length (cm) \n",
    "1. sepal width (cm) \n",
    "1. petal length (cm) \n",
    "1. petal width (cm) \n",
    "\n",
    "**Classes:**\n",
    "1. Iris Setosa \n",
    "1. Iris Versicolour \n",
    "1. Iris Virginica"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "The [seaborn library](https://stanford.edu/~mwaskom/software/seaborn/) is a visualization library. It can be installed from the terminal using \n",
    "```\n",
    "pip3 install seaborn\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "df = sns.load_dataset(\"iris\") # built-in dataset in seaborn \n",
    "sns.pairplot(df, hue=\"species\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# import data, scikit-learn also has this dataset built-in\n",
    "iris = load_iris()\n",
    "\n",
    "print( iris.feature_names )\n",
    "print( iris.target_names )\n",
    "\n",
    "# For easy plotting and interpretation, we only use first 2 features here. \n",
    "# We're throwing away useful information - don't do this at home! \n",
    "X = iris.data[:,:2]  \n",
    "y = iris.target\n",
    "\n",
    "# Create color maps\n",
    "cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF'])\n",
    "cmap_bold = ListedColormap(['#FF0000', '#00FF00', '#0000FF'])\n",
    "\n",
    "# plot data\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y,  marker=\"o\", cmap=cmap_bold, s=30)\n",
    "\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "\n",
    "plt.xlim(x_min, x_max)\n",
    "plt.ylim(y_min, y_max)    \n",
    "plt.title('Iris dataset')\n",
    "plt.xlabel( 'Sepal Length (cm)' )\n",
    "plt.ylabel( 'Sepal Width (cm)' )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "We see that it would be fairly easy to separate the \"red\" irises from the two classes. However, separating the \"green\" and \"blue\" ones would be a challenge. \n",
    "\n",
    "There are three classes, so we can't apply logistic regression. (This isn't completely true; there are extensions of logistic regression to handle more classes, but these are not very popular.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# set up the model, k-NN classification with k = ?  \n",
    "k = 20\n",
    "clf = KNeighborsClassifier( n_neighbors=k )\n",
    "clf.fit( X, y )\n",
    "\n",
    "# Plot classification \n",
    "xx, yy = np.meshgrid(np.linspace(x_min, x_max, 200),np.linspace(y_min, y_max, 200))\n",
    "zz = clf.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
    "plt.pcolormesh(xx, yy, zz, cmap=cmap_light)\n",
    "\n",
    "# Plot data\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap=cmap_bold,s=30)\n",
    "\n",
    "plt.title('Classification of Iris dataset using k-NN with k = '+ str(k))\n",
    "plt.xlabel( 'Sepal Length (cm)' )\n",
    "plt.ylabel( 'Sepal Width (cm)' )\n",
    "plt.xlim(x_min, x_max)\n",
    "plt.ylim(y_min, y_max)    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "print('Confusion Matrix:')\n",
    "y_pred = clf.predict(X)\n",
    "print(metrics.confusion_matrix(y_true = y, y_pred = y_pred))\n",
    "\n",
    "print('Accuracy = ', metrics.accuracy_score(y_true = y, y_pred = y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "**Some preliminary comments on the parameter, $k$:** \n",
    "\n",
    "- For k large (say $k=100$), the *decision boundary* (boundary between classes) becomes more smooth. The model is not very complex - it could basically be described by a line. The model is stable in the sense that if the data were to change slightly, the model wouldn't change much. (There are many voters.) Since the model doesn't depend on the data very much, we might expect that it would *generalize* to new data points. \n",
    "\n",
    "- For k small (say $k=1$), the decision boundary is very wiggly. The model is very complex - it definitely isn't described by a single line. If the model is unstable in the sense that if the data were to change slightly, the model would change quite a bit. Since the model is very dependent on the dataset, we would say that it wouldn't generalize to new data points well. In this case, we would say that the model has overfit the data. (We saw a similar phenomena in regression using high degree polynomials.) \n",
    "\n",
    "\n",
    "**Questions:**\n",
    "1. How to choose k? (more on this below)\n",
    "+ Which does a better job on the two moons dataset: k-NN or logistic regression?\n",
    "\n",
    "To the moons!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example: k-NN on the moons dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# moons\n",
    "X,y = make_moons( n_samples=500, random_state=1, noise=0.3 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "model = KNeighborsClassifier(n_neighbors = 20)\n",
    "model.fit(X, y)\n",
    "\n",
    "# Plot the data, color by class\n",
    "plt.scatter(X[y == 1, 0], X[y == 1, 1], color=\"darkblue\", marker=\"s\",label=\"class 1\")\n",
    "plt.scatter(X[y == 0, 0], X[y == 0, 1], color=\"darkred\", marker=\"o\",label=\"class 2\")\n",
    "plt.legend(scatterpoints=1)\n",
    "\n",
    "# Plot the predictions made by Logistic Regression\n",
    "x_min, x_max = X[:,0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.linspace(x_min, x_max, 200),np.linspace(y_min, y_max, 200))\n",
    "zz = model.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
    "plt.contourf(xx, yy, zz, cmap=ListedColormap(['DarkRed', 'DarkBlue']), alpha=.2)\n",
    "plt.contour(xx, yy, zz, colors=\"black\", alpha=1, linewidths=0.2) \n",
    "\n",
    "plt.xlim(x_min, x_max)\n",
    "plt.ylim(y_min, y_max)\n",
    "plt.xlabel('feature 1')\n",
    "plt.ylabel('feature 2')\n",
    "plt.title('Classification of Two Moons using k-NN')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "print('Confusion Matrix:')\n",
    "y_pred = model.predict(X)\n",
    "print(metrics.confusion_matrix(y_true = y, y_pred = y_pred))\n",
    "\n",
    "print('Accuracy = ', metrics.accuracy_score(y_true = y, y_pred = y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "For *good* choices of the parameter k, k-NN is a better classifier than logistic regression. Logistic regression suffers because the decision boundary isn't curved. For this reason, it is called a *linear classifier*.  (However there are extensions to logistic regression that allow the decision boundary to curve). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Model generalizability and cross-validation\n",
    "\n",
    "In classification, and other prediction problems, we would like to develop a model on a dataset, the *training dataset*, that will not only perform well on that dataset but on similar data that the model hasn't yet seen, the *testing dataset*. If a model satisfies this criterion, we say that it is *generalizable*. \n",
    "\n",
    "If a model has 100% accuracy on the training dataset ($k=1$ in k-NN) but doesn't generalize to new data, then it isn't a very good model. We say that this model has *overfit* the data. On the other hand, it isn't difficult to see that we could also *underfit* the data (taking $k$ large in k-NN). In this case, the model isn't complex enough to have good accuracy on the training dataset. \n",
    "\n",
    "**Cross-validation** is a general method for assessing how the results of a model (classification, regression,...) will *generalize* to an independent data set. In classification, cross-validation is a method for assessing how well the classification model will predict the class of points that weren't used to *train* the model. \n",
    "\n",
    "The idea of the method is simple: \n",
    "1. Split the dataset into two groups: the training dataset and the testing dataset. \n",
    "+ Train the model on the training dataset. \n",
    "+ Check the accuracy of the model on the testing dataset. \n",
    "\n",
    "In practice, you have to decide how to split the data into groups (i.e. how large the groups should be). You might also want to repeat the experiment so that the assessment doesn't depend on the way in which you split the data into groups. We'll worry about this in a later lecture. \n",
    "\n",
    "For now, I just want you to conceptually understand how generalizable k-NN is as we vary the parameter, k. \n",
    "\n",
    "<img src=\"https://cambridgecoding.files.wordpress.com/2016/03/figures_mod4_over_underfitting.png?w=1220&h=1612\" width=\"500\">\n",
    "\n",
    "$\\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad$ \n",
    "source: [this blog](https://blog.cambridgecoding.com/2016/03/24/misleading-modelling-overfitting-cross-validation-and-the-bias-variance-trade-off/)\n",
    "\n",
    "As the model becomes more complex (k decreases), the accuracy always increases for the training dataset. But, at some point, it starts to overfit the data and the accuracy decreases for the test dataset! Cross validation techniques will allow us to find the sweet-spot for the parameter k! (Think: Goldilocks and the Three Bears.)\n",
    "\n",
    "\n",
    "Let's see this concept for the two moons dataset. You can use the *train_test_split* function i scikit-learn to split the dataset into a training dataset and a test dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def detect_plot_dimension(X, h=0.02, b=0.05):\n",
    "    x_min, x_max = X[:, 0].min() - b, X[:, 0].max() + b\n",
    "    y_min, y_max = X[:, 1].min() - b, X[:, 1].max() + b\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "    dimension = xx, yy\n",
    "    return dimension\n",
    " \n",
    "def detect_decision_boundary(dimension, model):\n",
    "    xx, yy = dimension  # unpack the dimensions\n",
    "    boundary = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    boundary = boundary.reshape(xx.shape)  # Put the result into a color plot\n",
    "    return boundary\n",
    " \n",
    "def plot_decision_boundary(panel, dimension, boundary, colors=['#DADDED', '#FBD8D8']):\n",
    "    xx, yy = dimension  # unpack the dimensions\n",
    "    panel.contourf(xx, yy, boundary, cmap=ListedColormap(colors), alpha=1)\n",
    "    panel.contour(xx, yy, boundary, colors=\"g\", alpha=1, linewidths=0.5)  # the decision boundary in green\n",
    " \n",
    "def plot_dataset(panel, X, y, colors=[\"#EE3D34\", \"#4458A7\"], markers=[\"x\", \"o\"]):\n",
    "    panel.scatter(X[y == 1, 0], X[y == 1, 1], color=colors[0], marker=markers[0])\n",
    "    panel.scatter(X[y == 0, 0], X[y == 0, 1], color=colors[1], marker=markers[1])\n",
    " \n",
    "def calculate_prediction_error(model, X, y):\n",
    "    yPred = model.predict(X)\n",
    "    score = round(metrics.accuracy_score(y, yPred), 2)\n",
    "    return score\n",
    " \n",
    "def plot_prediction_error(panel, dimension, score, b=.3):\n",
    "    xx, yy = dimension  # unpack the dimensions\n",
    "    panel.text(xx.max() - b, yy.min() + b, ('%.2f' % score).lstrip('0'), size=15, horizontalalignment='right')\n",
    " \n",
    "def explore_fitting_boundaries(model, n_neighbors, datasets, width):  \n",
    "    # determine the height of the plot given the aspect ration of each panel should be equal\n",
    "    height = float(width)/len(n_neighbors) * len(datasets.keys())\n",
    " \n",
    "    nrows = len(datasets.keys())\n",
    "    ncols = len(n_neighbors)\n",
    " \n",
    "    # set up the plot\n",
    "    figure, axes = plt.subplots(\n",
    "        nrows,\n",
    "        ncols,\n",
    "        figsize=(width, height),\n",
    "        sharex=True,\n",
    "        sharey=True\n",
    "    )\n",
    " \n",
    "    dimension = detect_plot_dimension(X, h=0.02)  # the dimension each subplot based on the data\n",
    " \n",
    "    # Plotting the dataset and decision boundaries\n",
    "    i = 0\n",
    "    for n in n_neighbors:\n",
    "        model.n_neighbors = n\n",
    "        model.fit(datasets[\"Training Set\"][0], datasets[\"Training Set\"][1])\n",
    "        boundary = detect_decision_boundary(dimension, model)\n",
    "        j = 0\n",
    "        for d in datasets.keys():\n",
    "            try:\n",
    "                panel = axes[j, i]\n",
    "            except (TypeError, IndexError):\n",
    "                if (nrows * ncols) == 1:\n",
    "                    panel = axes\n",
    "                elif nrows == 1:  # if you only have one dataset\n",
    "                    panel = axes[i]\n",
    "                elif ncols == 1:  # if you only try one number of neighbors\n",
    "                    panel = axes[j]\n",
    "            plot_decision_boundary(panel, dimension, boundary)  # plot the decision boundary\n",
    "            plot_dataset(panel, X=datasets[d][0], y=datasets[d][1])  # plot the observations\n",
    "            score = calculate_prediction_error(model, X=datasets[d][0], y=datasets[d][1])\n",
    "            plot_prediction_error(panel, dimension, score, b=0.2)  # plot the score\n",
    " \n",
    "            # make compacted layout\n",
    "            panel.set_frame_on(False)\n",
    "            panel.set_xticks([])\n",
    "            panel.set_yticks([])\n",
    " \n",
    "            # format the axis labels\n",
    "            if i == 0:\n",
    "                panel.set_ylabel(d)\n",
    "            if j == 0:\n",
    "                panel.set_title('k={}'.format(n))\n",
    "            j += 1\n",
    "        i += 1   \n",
    " \n",
    "    plt.subplots_adjust(hspace=0, wspace=0)  # make compacted layout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Split into training and test sets\n",
    "XTrain, XTest, yTrain, yTest = train_test_split( X, y, random_state=1, test_size=0.5 )\n",
    "\n",
    "# specify the model and settings\n",
    "model = KNeighborsClassifier()\n",
    "n_neighbors = [200, 99, 50, 23, 11, 1]\n",
    "datasets = {\n",
    "    \"Training Set\": [XTrain, yTrain],\n",
    "    \"Test Set\": [XTest, yTest]\n",
    "}\n",
    "width = 20\n",
    "\n",
    "explore_fitting_boundaries(model=model, n_neighbors=n_neighbors, datasets=datasets, width=width)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into training and test sets\n",
    "XTrain, XTest, yTrain, yTest = train_test_split(X, y, random_state=1, test_size=0.5)\n",
    "\n",
    "# specify the model and settings\n",
    "model = KNeighborsClassifier()\n",
    "n_neighbors = [40, 30, 25, 20, 15]\n",
    "datasets = {\n",
    "    \"Training Set\": [XTrain, yTrain],\n",
    "    \"Test Set\": [XTest, yTest]\n",
    "}\n",
    "width = 20\n",
    "\n",
    "explore_fitting_boundaries(model=model, n_neighbors=n_neighbors, datasets=datasets, width=width)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Conclusions and take-away\n",
    "\n",
    "1. k-NN is a very simple method that can be used for classification. (It can be used for regression too! How?)  \n",
    "\n",
    "+ Model accuracy (measured on the training dataset) and generalizability (measured on the testing dataset) are both important and often in contention with one another. Model accuracy can be measured using the confusion matrix or the percent of misclassified samples. Generalizability can be measured via cross validation. \n",
    "\n",
    "+ Picking parameters in models (such as k in k-NN) is non-trivial, but can be done via cross validation. \n",
    "\n",
    "\n",
    "### Classification method preview\n",
    "For a quick preview of other classification methods, see the comparison [here](http://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html).\n",
    "\n",
    "![](http://scikit-learn.org/stable/_images/sphx_glr_plot_classifier_comparison_001.png)\n",
    "\n",
    "We will look at some other classification methods such as: SVM (Support Vector Machine - Rubber Sheet Stretching), Decision Trees, Neural Nets.\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "interpreter": {
   "hash": "8fcd46d531329f49b73a0948faa8aed429eb8dafd35203d9948e8358a307ba0e"
  },
  "kernelspec": {
   "display_name": "Python 3.10.3 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
