{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You conduct an A/B test for your website by showing A to some of your users, and B to others.\n",
    "\n",
    " Ad Clicks\tTotal Views\n",
    " A\t500\t1000\n",
    " B\t550\t1000\n",
    "\n",
    " It looks like B is better than A...\n",
    "\n",
    "# Here the Null Hypothesis is A and B are equal (pA = pB)\n",
    "# so that the alternative hypothesis is B is better than A( pA != pB)\n",
    "## Statistical inference for a difference between population proportions\n",
    "We consider comparing the population proportions of two different populations. \n",
    "\n",
    "We make the following definitions:\n",
    "- $N_A$ (1000) is the number of surveyed people from population $A$  \n",
    "- $n_A$(500) is the number of successes from population $A$\n",
    "- $p_A = n_A/N_A$ ( 500/1000 == 0.50) is the proportion of successes from population $A$\n",
    "\n",
    "Similarly, we define \n",
    "- $N_B$(1000) is the number of surveyed people from population $B$  \n",
    "- $n_B$(550) is the number of successes from population $B$\n",
    "- $p_B = n_B/N_B$  (550/1000 == 0.55)is the proportion of successes from population $B$\n",
    "\n",
    "We make the null hypothesis:\n",
    "$$\n",
    "H_0\\colon \\text{$p_A$ and $p_B$ are not same, that is, } p_A - p_B = 0.05\n",
    "$$\n",
    "That is, the proportion of successes in the two populations is the same. \n",
    "\n",
    "We'll take it as a fact (see Devore Ch. 9.4 or Math 3070) that: \n",
    "- $n_A/N_A$ is approximately a normal random variable with mean $p_A$ and variance $\\sigma_A^2 = p_A(1-p_A)/N_A$  (0.00025)\n",
    "- $n_B/N_B$ is approximately a normal random variable with mean $p_B$ and variance $\\sigma_B^2 = p_B(1-p_B)/N_B$ (0.0002475)\n",
    "- $n_A/N_A - n_B/N_B$ is approximately a normal random variable with mean $\\mu = 0$ and variance $\\sigma^2 = \\sigma_A^2 + \\sigma_B^2$. \n",
    "\n",
    "Here $\\hat{p} = \\frac{N_A}{N_A + N_B}p_A + \\frac{N_B}{N_A + N_B}p_B$ and $\\hat{q} = 1-\\hat{p}$. \n",
    "\n",
    "$\\hat{p} = 0.525\n",
    "    \n",
    "$\\hat{q} = 1-\\hat{p}$. == 0.475\n",
    " \n",
    "- The test statistic called the *two-proportion z-value* \n",
    "$$\n",
    "Z = \\frac{p_A - p_B}{\\sqrt{\\hat{p} \\hat{q} \\left( \\frac{1}{N_A} + \\frac{1}{N_B} \\right)}}.\n",
    "$$\n",
    "is approximately  distributed according to the standard normal distribution when $H_0$ is true. Here $\\hat{p} = \\frac{N_A}{N_A + N_B}p_A + \\frac{N_B}{N_A + N_B}p_B$ and $\\hat{q} = 1-\\hat{p}$. \n",
    "\n",
    "From the data, we estimate the mean, $\\mu$, to be  $p_A - p_B$. \n",
    "    \n",
    "\n",
    " Perform a hypothesis test using the two-proportion z-value test.  See the \"pooled\" version from the table hereLinks to an external site. statistic at both the 5% and 1% signficance levels.\n",
    "\n",
    " Begin by formulating and writing down your null hypothesis, then compute the Z value for this trial, and its associated p-value.\n",
    "\n",
    " Note: the Z value you compute is assumed to be distributed according to the standard normal distribution (mean = 0, variance = 1). \n",
    "We'll consider the \"1-sided\" p-value which meaures the probability of a sample having a Z value that is more extreme in only one tail (to get the 2-sided value, you'd just multiply by 2).\n",
    "\n",
    " What can you conclude?\n",
    "\n",
    " Note, to help you understand your results, you might want to tweak the number of clicks for b to see how that changes your results. For example, what do you expect to happen to your p value if b is clicked 560 or 570 times? This is JUST for understanding the results, and shouldn't bias your decision one way or another!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports and setup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sc\n",
    "import scipy.stats as stats\n",
    "import statsmodels.formula.api as sm\n",
    "from sklearn import linear_model\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "\n",
    "\n",
    "def hypothesis_test(nA,nB):\n",
    "    Na = 1000\n",
    "    pA = nA/Na\n",
    "   \n",
    "    Nb = 1000\n",
    "    pB = nB/Nb\n",
    "    \n",
    "    phat = Na*pA/(Na+Nb) + Nb*pB/(Na+Nb)\n",
    "    qhat = 1-phat\n",
    "    z = (pA - pB)/math.sqrt(phat*qhat*(1/Na + 1/Nb)) \n",
    "\n",
    "    p_value = stats.norm.cdf(z)\n",
    "    p_value = 2 * p_value\n",
    "\n",
    "    alpha = 0.05\n",
    "\n",
    "    if(p_value < alpha):\n",
    "        result_5 = \"The probability significant value \"+str(alpha * 100)+\"% is less than the P-value, then we conclude that the null hypothesis was incorrect, so we reject it. So Here B is better than A\"\n",
    "    else:\n",
    "       result_5 = \"The probability significant value \"+str(alpha * 100)+\"% is greater than the P-value, then we conclude that the null hypothesis was correct. So Here B and A have same impact\"\n",
    "    \n",
    "    print(result_5)\n",
    "    alpha = 0.01\n",
    "    if(p_value < alpha):\n",
    "        result_1 = \"The probability significant value \"+str(alpha * 100)+\"% is less than the P-value, then we conclude that the null hypothesis was incorrect, so we reject it. So Here B is better than A\"\n",
    "    else:\n",
    "        result_1 = \"The probability significant value \"+str(alpha * 100)+\"% is greater than the P-value, then we conclude that the null hypothesis was correct. So Here B and A have same impact\"\n",
    "    print(result_1)  \n",
    "    return result_5, result_1\n",
    "\n",
    "\n",
    "# hypothesis_test for nA = 500 and nB = 550\n",
    "hypothesis_test_1 = hypothesis_test(500,550)\n",
    "\n",
    "# hypothesis_test for nA = 500 and nB = 560\n",
    "hypothesis_test_2 = hypothesis_test(500,560)\n",
    "\n",
    "\n",
    "# hypothesis_test for nA = 500 and nB = 570\n",
    "hypothesis_test_3 = hypothesis_test(500,570)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 2: Regression of real estate data\n",
    "\n",
    "#Task 1: Import the Data\n",
    "df1 = pd.read_csv('realEstate1.csv')\n",
    "df2 = pd.read_csv('realEstate2.csv')\n",
    "\n",
    "df = pd.concat([df1, df2], axis=0)\n",
    "\n",
    "\n",
    "#Task 2: Clean the Data\n",
    "print(df.shape)\n",
    "df = df[(df['LstPrice'] >= 200000) & (df['LstPrice'] <= 1000000)]\n",
    "print(df.shape)\n",
    "df = df[['Acres', 'Deck', 'GaragCap', 'Latitude', 'Longitude', 'LstPrice', 'Patio', 'PkgSpacs', 'PropType', 'SoldPrice', 'Taxes', 'TotBed', 'TotBth', 'TotSqf', 'YearBlt']]\n",
    "print(df.shape)\n",
    "\n",
    "df['TotSqf'] = df['TotSqf'].str.replace(',','').astype(int)\n",
    "df['Prop_Type_SingleFamily'] = (df['PropType'] == 'Single Family').astype(int)\n",
    "\n",
    "df = df[df['Longitude'] != 0]\n",
    "print(df.shape)\n",
    "tax_idx = df['Taxes'] == df['Taxes'].max()\n",
    "df = df[~tax_idx]\n",
    "tax_idx = df['Taxes'] == df['Taxes'].max()\n",
    "df = df[~tax_idx]\n",
    "print(df.shape)\n",
    "\n",
    "#Task 3: Exploratory Data Analysis\n",
    "print(\"After Cleaning up the data we are left with \"+str(df.shape[0]) +\" houses and \" +str(df.shape[1])+\" variables\")\n",
    "df['PropType'].value_counts().plot(kind='bar')\n",
    "plt.show()\n",
    "\n",
    "#Compute the correlation matrix and use a heat map to visualize the correlation coefficients\n",
    "original_categories = df['PropType'].astype('category').cat.categories\n",
    "df['PropType'] = df['PropType'].astype('category').cat.codes\n",
    "\n",
    "corr = df.corr()\n",
    "plt.pcolor(corr, vmin=-1, vmax=1, cmap='coolwarm')\n",
    "plt.colorbar()\n",
    "plt.xticks(range(len(corr)), corr.columns, rotation=90)\n",
    "plt.yticks(range(len(corr)), corr.columns)\n",
    "plt.show()\n",
    "\n",
    "df['PropType'] = df['PropType'].map(dict(enumerate(original_categories)))\n",
    "\n",
    "# Create a subset of the columns\n",
    "subset_cols = ['Acres', 'LstPrice', 'SoldPrice', 'Taxes', 'TotBed', 'TotBth', 'TotSqf', 'YearBlt']\n",
    "\n",
    "\n",
    "\n",
    "# Make a scatter plot matrix\n",
    "for col1 in subset_cols:\n",
    "    for col2 in subset_cols:\n",
    "        if col1 != col2:\n",
    "            corr_coeff = df[col1].corr(df[col2])\n",
    "            if abs(corr_coeff) > 0.75:\n",
    "                print('There is a strong correlation between {} and {} (correlation coefficient = {})'.format(col1, col2, corr_coeff))\n",
    "            plt.scatter(df[col1], df[col2], alpha=0.5)\n",
    "            plt.xlabel(col1)\n",
    "            plt.ylabel(col2)\n",
    "            plt.show()\n",
    "\n",
    "#Describing your findings.\n",
    "#LstPrice and SoldPrice have highest R^2 Value which includes 99% of data\n",
    "# TotSqf and Taxes are other two variables which have good correlation coefficient , So these can be considered to build a model to determine best Sold Price\n",
    "\n",
    "#Task 4: Geospatial Plot\n",
    "\n",
    "# Create a scatterplot of latitude vs longitude\n",
    "plt.scatter(df['Latitude'], df['Longitude'], c=df['LstPrice'], cmap='coolwarm' )\n",
    "plt.colorbar(label='List Price')\n",
    "plt.xlabel('Latitude')\n",
    "plt.ylabel('Longitude')\n",
    "plt.title('House Prices in Salt Lake City')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#The Price Depends on the Longitude and Latitude of Salt Lake City From the graph The highest price houses are located at Longitude (111.85) and Latitude (40.970)\n",
    "# Most of the Houses are in the region from (-111.90 to -111.86) Longitude and (40.770 to 40.780) Which are of average cost and Houses which are far from city like towards airport have higher cost\n",
    "\n",
    "#Task 5: Simple Linear Regression\n",
    "linear_model = sm.ols(formula=\"SoldPrice ~ LstPrice\" , data = df).fit()\n",
    "\n",
    "print(f\"The R Sqaure value of linear_model is \" +str(round(linear_model.rsquared,4)))\n",
    "# Interpretation of R-squared: The R-squared value represents the proportion of the variance in the Sold price that can be explained by the List price. \n",
    "# In this case, the R-squared value indicates how well the List price predicts the variation in the Sold price. A higher R-squared value suggests that the List price is a good predictor of the Sold price.\n",
    "\n",
    "\n",
    "print(f\"The beta_1 of linear_model is \" +str(round(linear_model.rsquared,4)))\n",
    "# Interpretation of beta_1: The beta_1 coefficient represents the change in the Sold price for each unit increase in the List price which is 0.9444\n",
    "# In other words, it quantifies the average change in the Sold price associated with a one-unit increase in the List price, holding other variables constant.\n",
    "\n",
    "\n",
    "plt.scatter(df['SoldPrice'],df['LstPrice'],color='black',linewidth=1)\n",
    "plt.xlabel('SoldPrice'); plt.ylabel('LstPrice')\n",
    "plt.show()\n",
    "\n",
    "#Task 6: Multilinear Regression\n",
    "\n",
    "subset_cols = ['Acres', 'Taxes','Latitude * Longitude', 'TotBed', 'TotBth', 'TotSqf', 'YearBlt','GaragCap']\n",
    "multiLinear_model = sm.ols(formula=\"SoldPrice ~ \" + \" + \".join(subset_cols), data=df).fit()\n",
    "\n",
    "\n",
    "#Often the price per square foot for a house is advertised. \n",
    "# Is this what the coefficient for TotSqf is measuring? Provide an interpretation for the coefficient for TotSqf.\n",
    "print(f\"Here the coefficient for TotSqf is positive and hence a larger Total Square Feet is associated with a higher Sold Price on average \"+ str(round(multiLinear_model.params['TotSqf'],4)))\n",
    "\n",
    "#Estimate the value that each Garage space adds to a house.\n",
    "\n",
    "garage_coefficient = multiLinear_model.params['GaragCap']\n",
    "\n",
    "if garage_coefficient > 0 and multiLinear_model.pvalues['GaragCap'] < 0.05:\n",
    "    print(\"The coefficient for Garage is positive and statistically significant.\")\n",
    "    print(\"Having an additional Garage space increases the Sold Price of the house, on average.\")\n",
    "else:\n",
    "    print(\"The coefficient for Garage is either not positive or not statistically significant.\")\n",
    "    print(\"The presence of an additional Garage space may not have a significant impact on the Sold Price.\")\n",
    "\n",
    "# Does latitude or longitude have an impact on house price? Explain.    \n",
    "latitude_coefficient = multiLinear_model.params['Latitude:Longitude']\n",
    "\n",
    "if latitude_coefficient > 0 and multiLinear_model.pvalues['Latitude:Longitude'] < 0.05:\n",
    "    print(\"The coefficient for Latitude & Longitude is positive and statistically significant.\")\n",
    "    print(\"Based on   Latitude & Longitude the Sold Price of the house increases, on average.\")\n",
    "else:\n",
    "    print(\"The coefficient for  Latitude & Longitude is either not positive or not statistically significant.\")\n",
    "    print(\"The presence of  Latitude & Longitude may not have a significant impact on the Sold Price.\")\n",
    "\n",
    "\n",
    "#If we wanted to start a 'house flipping' company, we'd have to be able to do a better job of predicting the sold price than the list price does.\n",
    "#  How does your model compare?\n",
    "\n",
    "#The above model gives us the best Sold price based on the Variables for a already built house for resale as it is , But \"House flipping\" company either rebuilt or renovates the house\n",
    "# We don't have a data of Labour cost and construction cost of the new house and renewed house Hence above model cannot predict  the better sold price for house filpping company \n",
    "\n",
    "\n",
    "\n",
    "#Task 7: Incorporating a Categorical Variable\n",
    "regression_model_1 = sm.ols(formula=\"SoldPrice ~ Prop_Type_SingleFamily\", data=df).fit()\n",
    "p_value = regression_model_1.pvalues['Prop_Type_SingleFamily']\n",
    "\n",
    "if p_value < 0.05:\n",
    "    print(\"Property type is significant in predicting the sold price.\")\n",
    "else:\n",
    "    print(\"Property type is not significant in predicting the sold price.\")\n",
    "\n",
    "regression_model_2 = sm.ols(formula=\"SoldPrice ~ Prop_Type_SingleFamily + TotSqf\", data=df).fit()\n",
    "p_value2 = regression_model_2.pvalues['Prop_Type_SingleFamily']\n",
    "\n",
    "if p_value2 < p_value:\n",
    "    print(\"When considering total square footage, property type is no longer predictive.\")\n",
    "else:\n",
    "    print(\"When considering total square footage, property type remains predictive.\")\n",
    "\n",
    "\n",
    "list = df['PropType'].unique()\n",
    "df['PropType'] = df['PropType'].astype('category')\n",
    "# Create a scatterplot of TotSqf vs. SoldPrice\n",
    "plt.scatter(df['TotSqf'], df['SoldPrice'],  c=df['PropType'].cat.codes, alpha=0.5)\n",
    "plt.xlabel('TotSqf')\n",
    "plt.ylabel('SoldPrice')\n",
    "plt.title('TotSqf vs. SoldPrice')\n",
    "plt.legend(list, loc='upper left')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
